# 随机变量的自信息

X、Y、Z：随机变量

x、y、z：随机变量的具体取值

$x、y、z$：集合

$|x|、|y|、|z|$：集合的势（集合中元素的个数）

$P_{X}(x)=\mathbb{P}(X=x)$：随机变量的概率分布

$P_{XY}(x,y)=\mathbb{P}(X=x,Y=y)$：联合概率<u>（随机过程）</u>

$X^{n}=X_{1},X_{2}...X_{n}$：长度为n的随机变量序列

$x^{n}=x_{1},x_{2}...x_{n}$：长度为n的数值序列

$X_{i}^{j}=X_{i},X_{i+1}...X_{j}$：长度为$（j-i+1）$的随机变量序列

$X^{n}=(X_{1},X_{2}...X_{n})$：维度为n的随机矢量

$x^{n}=(x_{1},x_{2}...x_{n})$：维度为n的数值矢量

$X_{i}^{j}=(X_{i},X_{i+1}...X_{j})$：维度为$（j-i+1）$的随机矢量

## 信息是对不确定性的消除

## 随机变量的自信息

### 四个基本问题：

- 随机性与概率的关系：概率越低，随机事件所提$$供的自信息越大；反之，随机事件所提供的自信息越小。
- 概率为1的事件的信息量：概率为1的事件为确定性事件，它所能够提供的信息量为0，因为没有任何的不确定性被消除。
- 概率为0的事件的信息量：随着概率变得越来越小，它所能够提供的信息量越来越大，概率为0的事件的信息量$\rightarrow \infty $。
- 两个独立事件的联合信息量：为它们各自自信息量的代数和。

​                                                                                              ↓

设$a_{1},a_{2}$为两个随机事件，

（1）若$P(a_{1})>P(a_{2})$，则$f(a_{1})<f(a_{2})$

（2）若$P(a_{1})=1$，则$f(a_{1})=0$

（3）若$P_{a}=0$，则$f(a_{1})=∞$

（4）如果$a_{1},a_{2}$为独立事件，则$f(a_{1},a_{2})=f(a_{1})+f(a_{2})$

## 自信息

$$
I(a_{i})=log\frac{1}{P(a_{i})}
$$

这是一个减函数，当$P(a_{i})=0$时，$I(a_{i})=∞$；当$P(a_{i})=1$时，$I(a_{i})=0$。

### 对数底与信息的单位

​        以2为底：  bit    （binary unit）

​        以e为底：  nat   （nature unit）

​	    以10为底：Hart （Hartley）

### 换算关系：

​	1 nat  =1.44bit

​	1 Hart=3.32bit

一般不加说明，取以2为底。

![在这里插入图片描述](https://img-blog.csdnimg.cn/a1fde713cb1949d2a9bad8bc23506545.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56iL5bqP5ZGY5bCP5YuH,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center)



